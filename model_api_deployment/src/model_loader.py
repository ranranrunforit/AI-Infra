"""
Model Loader Module

This module handles loading pre-trained models, preprocessing images,
and generating predictions.

"""

import logging
from typing import List, Dict, Optional, Tuple
from PIL import Image
import torch
import torch.nn as nn
from torchvision import models, transforms
import requests

# TODO: Import config after implementing config.py
from config import Config

# Set up logging
logger = logging.getLogger(__name__)


class ModelLoader:
    """
    Manages ML model lifecycle and inference.

    This class handles:
    - Loading pre-trained models from torchvision
    - Image preprocessing (resize, normalize, etc.)
    - Running inference
    - Post-processing predictions
    - Loading ImageNet class labels

    Example:
        >>> loader = ModelLoader(model_name='resnet50')
        >>> loader.load()
        >>> image = Image.open('dog.jpg')
        >>> predictions = loader.predict(image, top_k=5)
        >>> print(predictions[0])
        {'class': 'golden_retriever', 'confidence': 0.89, 'rank': 1}
    """

    def __init__(self, model_name: str = "resnet50", device: str = "cpu"):
        """
        Initialize ModelLoader.

        TODO: Implement initialization
        - Store model_name and device
        - Initialize model to None (will be loaded later)
        - Initialize transform to None
        - Initialize class_labels to None
        - Set up any other instance variables needed

        Args:
            model_name: Name of model to load ('resnet50' or 'mobilenet_v2')
            device: Device to run inference on ('cpu' or 'cuda')
        """
        self.model_name: str = model_name
        self.device: str = device
        self.model: Optional[nn.Module] = None
        self.transform: Optional[transforms.Compose] = None
        self.class_labels: Optional[Dict[int, str]] = None

        # TODO: Add any additional initialization
        logger.info(f"ModelLoader initialized with model={model_name}, device={device}")

    def load(self) -> None:
        """
        Load model weights and prepare for inference.

        TODO: Implement model loading
        1. Load the model based on self.model_name
           - Use torchvision.models.resnet50() or mobilenet_v2()
           - Set pretrained=True (or weights='DEFAULT' in newer PyTorch)
        2. Move model to self.device
        3. Set model to evaluation mode (model.eval())
        4. Create preprocessing transform pipeline
        5. Load ImageNet class labels
        6. Log success

        Raises:
            ValueError: If model_name is not supported
            RuntimeError: If model loading fails

        Example:
            >>> loader = ModelLoader()
            >>> loader.load()
            >>> assert loader.model is not None
        """
        logger.info(f"Loading model: {self.model_name}")

        # TODO: Load model based on model_name
        # if self.model_name == "resnet50":
        #     self.model = models.resnet50(pretrained=True)
        # elif self.model_name == "mobilenet_v2":
        #     self.model = models.mobilenet_v2(pretrained=True)
        # else:
        #     raise ValueError(f"Unsupported model: {self.model_name}")

        # TODO: Move model to device
        # self.model = self.model.to(self.device)

        # TODO: Set model to evaluation mode
        # self.model.eval()

        # TODO: Create preprocessing pipeline
        # self.transform = self._create_transform()

        # TODO: Load class labels
        # self.class_labels = self._load_imagenet_labels()

        try:
            # Load model based on model_name
            if self.model_name == "resnet50":
                self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
            elif self.model_name == "mobilenet_v2":
                self.model = models.mobilenet_v2(
                    weights=models.MobileNet_V2_Weights.DEFAULT
                )
            else:
                raise ValueError(
                    f"Unsupported model: {self.model_name}. "
                    f"Use 'resnet50' or 'mobilenet_v2'"
                )
            
            # Move model to device
            self.model = self.model.to(self.device)
            
            # Set model to evaluation mode (disable dropout, batch norm)
            self.model.eval()
            
            # Create preprocessing pipeline
            self.transform = self._create_transform()
            
            # Load ImageNet class labels
            self.class_labels = self._load_imagenet_labels()
            
            logger.info(f"Model {self.model_name} loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            raise RuntimeError(f"Model loading failed: {e}")

    def _create_transform(self) -> transforms.Compose:
        """
        Create image preprocessing transform pipeline.

        TODO: Implement preprocessing pipeline
        - Resize to 256x256 (then center crop to 224x224)
        - Center crop to 224x224
        - Convert to tensor
        - Normalize with ImageNet mean and std
          Mean: [0.485, 0.456, 0.406]
          Std: [0.229, 0.224, 0.225]

        Returns:
            Composed transform pipeline

        Example:
            >>> loader = ModelLoader()
            >>> transform = loader._create_transform()
            >>> image = Image.open('dog.jpg')
            >>> tensor = transform(image)
            >>> tensor.shape
            torch.Size([3, 224, 224])
        """
        # TODO: Implement transform pipeline
        return transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])
        

    def _load_imagenet_labels(self) -> Dict[int, str]:
        """
        Load ImageNet class labels.

        TODO: Implement label loading
        - Download labels from IMAGENET_LABELS_URL (use requests library)
        - Parse text file (one label per line)
        - Create dictionary mapping index to label
        - Handle download failures gracefully
        - Consider caching labels locally

        Returns:
            Dictionary mapping class index to label name

        Raises:
            RuntimeError: If labels cannot be loaded

        Example:
            >>> loader = ModelLoader()
            >>> labels = loader._load_imagenet_labels()
            >>> labels[207]
            'golden retriever'
        """
        # TODO: Implement label loading
        # HINT: Labels file has 1000 lines, one label per line
        # Index 0 = first line, index 999 = last line
        #
        # try:
        #     url = "https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
        #     response = requests.get(url, timeout=10)
        #     response.raise_for_status()
        #     labels = response.text.strip().split('\n')
        #     return {i: label.strip() for i, label in enumerate(labels)}
        # except Exception as e:
        #     logger.error(f"Failed to load ImageNet labels: {e}")
        #     raise RuntimeError("Could not load class labels")
        try:
            url = "https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
            logger.info(f"Downloading ImageNet labels from {url}")
            
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            labels = response.text.strip().split('\n')
            label_dict = {i: label.strip() for i, label in enumerate(labels)}
            
            logger.info(f"Loaded {len(label_dict)} ImageNet labels")
            return label_dict
            
        except Exception as e:
            logger.error(f"Failed to load ImageNet labels: {e}")
            raise RuntimeError(f"Could not load class labels: {e}")

    def preprocess(self, image: Image.Image) -> torch.Tensor:
        """
        Preprocess image for model inference.

        TODO: Implement preprocessing
        - Validate image is not None
        - Convert image to RGB (handles grayscale and RGBA)
        - Apply transform pipeline
        - Add batch dimension (unsqueeze)
        - Move to device
        - Return preprocessed tensor

        Args:
            image: PIL Image object

        Returns:
            Preprocessed tensor with shape (1, 3, 224, 224)

        Raises:
            ValueError: If image is invalid or preprocessing fails

        Example:
            >>> loader = ModelLoader()
            >>> loader.load()
            >>> image = Image.open('dog.jpg')
            >>> tensor = loader.preprocess(image)
            >>> tensor.shape
            torch.Size([1, 3, 224, 224])
        """
        # TODO: Implement preprocessing
        # Validate input
        # if image is None:
        #     raise ValueError("Image cannot be None")

        # try:
        #     # Convert to RGB (handles grayscale and RGBA)
        #     if image.mode != 'RGB':
        #         image = image.convert('RGB')
        #
        #     # Apply transforms
        #     tensor = self.transform(image)
        #
        #     # Add batch dimension
        #     tensor = tensor.unsqueeze(0)
        #
        #     # Move to device
        #     tensor = tensor.to(self.device)
        #
        #     return tensor
        # except Exception as e:
        #     logger.error(f"Preprocessing failed: {e}")
        #     raise ValueError(f"Failed to preprocess image: {e}")
        if image is None:
            raise ValueError("Image cannot be None")
        
        try:
            # Convert to RGB (handles grayscale and RGBA)
            if image.mode != 'RGB':
                logger.debug(f"Converting image from {image.mode} to RGB")
                image = image.convert('RGB')
            
            # Apply transforms
            tensor = self.transform(image)
            
            # Add batch dimension
            tensor = tensor.unsqueeze(0)
            
            # Move to device
            tensor = tensor.to(self.device)
            
            return tensor
            
        except Exception as e:
            logger.error(f"Preprocessing failed: {e}")
            raise ValueError(f"Failed to preprocess image: {e}")

    def predict(self, image: Image.Image, top_k: int = 5) -> List[Dict[str, any]]:
        """
        Generate top-K predictions for image.

        TODO: Implement prediction
        1. Validate model is loaded
        2. Preprocess image
        3. Run inference (with torch.no_grad())
        4. Apply softmax to get probabilities
        5. Get top-K predictions
        6. Map indices to class labels
        7. Format as list of dictionaries
        8. Return predictions

        Args:
            image: PIL Image object to classify
            top_k: Number of top predictions to return (default: 5)

        Returns:
            List of prediction dictionaries with keys:
            - 'class': class label (str)
            - 'confidence': probability score (float)
            - 'rank': rank in predictions (int, 1-based)

        Raises:
            RuntimeError: If model not loaded
            ValueError: If prediction fails

        Example:
            >>> loader = ModelLoader()
            >>> loader.load()
            >>> image = Image.open('dog.jpg')
            >>> predictions = loader.predict(image, top_k=5)
            >>> len(predictions)
            5
            >>> predictions[0]['class']
            'golden_retriever'
            >>> 0 <= predictions[0]['confidence'] <= 1
            True
        """
        # TODO: Implement prediction
        # Validate model is loaded
        # if self.model is None:
        #     raise RuntimeError("Model not loaded. Call load() first.")

        # try:
        #     # Preprocess image
        #     tensor = self.preprocess(image)
        #
        #     # Run inference (no gradient computation needed)
        #     with torch.no_grad():
        #         outputs = self.model(tensor)
        #
        #     # Apply softmax to get probabilities
        #     probabilities = torch.nn.functional.softmax(outputs[0], dim=0)
        #
        #     # Get top-K predictions
        #     top_probs, top_indices = torch.topk(probabilities, top_k)
        #
        #     # Format results
        #     predictions = []
        #     for rank, (prob, idx) in enumerate(zip(top_probs, top_indices), start=1):
        #         predictions.append({
        #             'class': self.class_labels[idx.item()],
        #             'confidence': float(prob.item()),
        #             'rank': rank
        #         })
        #
        #     return predictions
        #
        # except Exception as e:
        #     logger.error(f"Prediction failed: {e}")
        #     raise ValueError(f"Failed to generate predictions: {e}")
        # Validate model is loaded
        if self.model is None:
            raise RuntimeError("Model not loaded. Call load() first.")
        
        try:
            # Preprocess image
            tensor = self.preprocess(image)
            
            # Run inference (no gradient computation needed)
            with torch.no_grad():
                outputs = self.model(tensor)
            
            # Apply softmax to get probabilities
            probabilities = torch.nn.functional.softmax(outputs[0], dim=0)
            
            # Get top-K predictions
            top_probs, top_indices = torch.topk(probabilities, top_k)
            
            # Format results
            predictions = []
            for rank, (prob, idx) in enumerate(zip(top_probs, top_indices), start=1):
                predictions.append({
                    'class': self.class_labels[idx.item()],
                    'confidence': float(prob.item()),
                    'rank': rank
                })
            
            logger.debug(f"Generated {len(predictions)} predictions")
            return predictions
            
        except Exception as e:
            logger.error(f"Prediction failed: {e}")
            raise ValueError(f"Failed to generate predictions: {e}")

    def get_model_info(self) -> Dict[str, any]:
        """
        Get model metadata and information.

        TODO: Implement model info
        - Return dictionary with model metadata
        - Include: name, framework, version, input shape, output classes
        - Handle case where model not loaded

        Returns:
            Dictionary containing model metadata

        Example:
            >>> loader = ModelLoader()
            >>> loader.load()
            >>> info = loader.get_model_info()
            >>> info['name']
            'resnet50'
            >>> info['input_shape']
            [224, 224, 3]
        """
        # TODO: Implement model info
        # return {
        #     'name': self.model_name,
        #     'framework': 'pytorch',
        #     'version': torch.__version__,
        #     'input_shape': [224, 224, 3],
        #     'output_classes': 1000,
        #     'device': self.device,
        #     'loaded': self.model is not None
        # }
        return {
            'name': self.model_name,
            'framework': 'pytorch',
            'version': torch.__version__,
            'input_shape': [224, 224, 3],
            'output_classes': 1000,
            'device': self.device,
            'loaded': self.model is not None
        }

    def validate_image(self, image: Image.Image) -> Tuple[bool, Optional[str]]:
        """
        Validate image meets requirements.

        TODO: Implement image validation
        - Check image is not None
        - Check image dimensions are reasonable (< MAX_IMAGE_DIMENSION)
        - Check image mode is valid
        - Return (is_valid, error_message)

        Args:
            image: PIL Image to validate

        Returns:
            Tuple of (is_valid: bool, error_message: Optional[str])

        Example:
            >>> loader = ModelLoader()
            >>> image = Image.open('dog.jpg')
            >>> is_valid, error = loader.validate_image(image)
            >>> is_valid
            True
            >>> error
            None
        """
        # TODO: Implement validation
        # if image is None:
        #     return False, "Image is None"
        #
        # # Check dimensions
        # width, height = image.size
        # max_dim = 10000  # or get from config
        # if width > max_dim or height > max_dim:
        #     return False, f"Image dimensions too large: {width}x{height}"
        #
        # # Check mode
        # if image.mode not in ['RGB', 'RGBA', 'L', 'P']:
        #     return False, f"Unsupported image mode: {image.mode}"
        #
        # return True, None
        if image is None:
            return False, "Image is None"
        
        # Check dimensions
        width, height = image.size
        max_dim = 10000
        if width > max_dim or height > max_dim:
            return False, f"Image dimensions too large: {width}x{height} (max: {max_dim})"
        
        # Check mode
        valid_modes = ['RGB', 'RGBA', 'L', 'P']
        if image.mode not in valid_modes:
            return False, f"Unsupported image mode: {image.mode}"
        
        return True, None

    def __repr__(self) -> str:
        """
        String representation of ModelLoader.

        TODO: Implement __repr__
        - Return informative string about model state
        - Include model name, device, and loaded status

        Example:
            >>> loader = ModelLoader()
            >>> print(loader)
            ModelLoader(model='resnet50', device='cpu', loaded=False)
        """
        # TODO: Implement __repr__
        # loaded = self.model is not None
        # return f"ModelLoader(model='{self.model_name}', device='{self.device}', loaded={loaded})"
        loaded = self.model is not None
        return f"ModelLoader(model='{self.model_name}', device='{self.device}', loaded={loaded})"



# =========================================================================
# Helper Functions
# =========================================================================

def load_model_from_path(path: str, model_name: str, device: str = "cpu") -> nn.Module:
    """
    Load model from custom path (for advanced use).

    TODO: Implement custom model loading (OPTIONAL)
    - Load model from custom checkpoint file
    - Useful for loading fine-tuned models
    - This is optional for the basic project

    Args:
        path: Path to model checkpoint
        model_name: Model architecture name
        device: Device to load model on

    Returns:
        Loaded model

    Example:
        >>> model = load_model_from_path('/path/to/model.pth', 'resnet50')
    """
    # TODO: OPTIONAL - Implement custom model loading
    pass


def download_file(url: str, local_path: str, timeout: int = 30) -> bool:
    """
    Download file from URL to local path.

    TODO: Implement file download (OPTIONAL)
    - Download file with progress tracking
    - Useful for downloading custom models or labels
    - This is optional for the basic project

    Args:
        url: URL to download from
        local_path: Path to save file
        timeout: Download timeout in seconds

    Returns:
        True if successful, False otherwise

    Example:
        >>> success = download_file('https://example.com/model.pth', 'model.pth')
    """
    # TODO: OPTIONAL - Implement file download
    pass


# =========================================================================
# Usage Example (for testing during development)
# =========================================================================

if __name__ == "__main__":
    """
    Test model loader functionality.

    Run this file directly to test your implementation:
    $ python model_loader.py
    """
    # TODO: Test your implementation
    # Example:
    # import sys
    #
    # # Set up logging
    # logging.basicConfig(
    #     level=logging.INFO,
    #     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    # )
    #
    # try:
    #     # Initialize model loader
    #     loader = ModelLoader(model_name='resnet50', device='cpu')
    #     print(f"Initialized: {loader}")
    #
    #     # Load model
    #     print("Loading model...")
    #     loader.load()
    #     print("Model loaded successfully!")
    #
    #     # Get model info
    #     info = loader.get_model_info()
    #     print(f"Model info: {info}")
    #
    #     # Test with sample image (if available)
    #     if len(sys.argv) > 1:
    #         image_path = sys.argv[1]
    #         print(f"\nTesting with image: {image_path}")
    #         image = Image.open(image_path)
    #
    #         # Validate image
    #         is_valid, error = loader.validate_image(image)
    #         if not is_valid:
    #             print(f"Invalid image: {error}")
    #             sys.exit(1)
    #
    #         # Make prediction
    #         print("Generating predictions...")
    #         predictions = loader.predict(image, top_k=5)
    #
    #         print("\nTop 5 Predictions:")
    #         for pred in predictions:
    #             print(f"  {pred['rank']}. {pred['class']}: {pred['confidence']:.4f}")
    #     else:
    #         print("\nTo test with an image, run: python model_loader.py <image_path>")
    #
    # except Exception as e:
    #     print(f"Error: {e}")
    #     import traceback
    #     traceback.print_exc()
    #     sys.exit(1)
    
    import sys
    
    # Set up logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        # Initialize model loader
        loader = ModelLoader(model_name='resnet50', device='cpu')
        print(f"Initialized: {loader}")
        
        # Load model
        print("\nLoading model...")
        loader.load()
        print("✓ Model loaded successfully!")
        
        # Get model info
        info = loader.get_model_info()
        print(f"\nModel info:")
        for key, value in info.items():
            print(f"  {key}: {value}")
        
        # Test with sample image (if path provided)
        if len(sys.argv) > 1:
            image_path = sys.argv[1]
            print(f"\nTesting with image: {image_path}")
            image = Image.open(image_path)
            
            # Validate image
            is_valid, error = loader.validate_image(image)
            if not is_valid:
                print(f"✗ Invalid image: {error}")
                sys.exit(1)
            
            # Make prediction
            print("Generating predictions...")
            predictions = loader.predict(image, top_k=5)
            
            print("\nTop 5 Predictions:")
            for pred in predictions:
                print(f"  {pred['rank']}. {pred['class']}: {pred['confidence']:.4f}")
        else:
            print("\nTo test with an image, run:")
            print("  python src/model_loader.py <image_path>")
        
    except Exception as e:
        print(f"✗ Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
