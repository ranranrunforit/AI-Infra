services:
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.9.2
    container_name: mlflow-server
    ports:
      - "5001:5001"
    environment:
      - MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow/mlflow.db
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=/mlflow/artifacts
    volumes:
      - mlflow-data:/mlflow
    networks:
      - ml-network
    command: mlflow server --host 0.0.0.0 --port 5001
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5001/health')"]
      interval: 30s
      timeout: 10s
      retries: 3

  ml-api:
    image: ml-api:latest
    container_name: ml-api
    depends_on:
      mlflow:
        condition: service_healthy
    ports:
      - "5000:5000"
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow-server:5001
      - MODEL_NAME=image-classifier
      - MODEL_VERSION=latest
      - API_KEYS=test-key,admin-key
      - LOG_LEVEL=INFO
    networks:
      - ml-network
    restart: unless-stopped
    # 1. SERVICE LEVEL VOLUME (Uses a dash '-')
    # This mounts the volume into the container
    volumes:
      - mlflow-data:/mlflow

networks:
  ml-network:
    driver: bridge

# 2. TOP LEVEL VOLUME (No dash)
# This creates the physical volume on your disk
volumes:
  mlflow-data: