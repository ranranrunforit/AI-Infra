---
# Kubernetes Deployment for Ray Distributed Training
# Optimized for single GPU (RTX 5070) on Docker Desktop + WSL2
#
# PREREQUISITE: Install nvidia-device-plugin first!
#   kubectl apply -f kubernetes/nvidia-device-plugin.yaml
#
# Usage:
#   kubectl apply -f kubernetes/deployment.yaml
#   kubectl get pods -n ray-cluster
#   kubectl logs -f deployment/ray-head -n ray-cluster

apiVersion: v1
kind: Namespace
metadata:
  name: ray-cluster
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ray-serviceaccount
  namespace: ray-cluster
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ray-config
  namespace: ray-cluster
data:
  NCCL_DEBUG: "INFO"
  NCCL_SOCKET_IFNAME: "eth0"
  RAY_BACKEND_LOG_LEVEL: "warning"
  PYTHONUNBUFFERED: "1"
  PYTHONPATH: "/app/src:/app"
---
# Ray Head Service — NodePort for dashboard and client access
apiVersion: v1
kind: Service
metadata:
  name: ray-head
  namespace: ray-cluster
spec:
  type: NodePort
  selector:
    app: ray
    component: head
  ports:
  - name: client
    port: 10001
    targetPort: 10001
    nodePort: 30001
  - name: dashboard
    port: 8265
    targetPort: 8265
    nodePort: 30265
  - name: redis
    port: 6379
    targetPort: 6379
    nodePort: 30379
  - name: metrics
    port: 8080
    targetPort: 8080
    nodePort: 30080
---
# Ray Head Deployment — runs on the GPU
# On a single-GPU laptop, the head node IS the worker
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ray-head
  namespace: ray-cluster
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ray
      component: head
  template:
    metadata:
      labels:
        app: ray
        component: head
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
    spec:
      serviceAccountName: ray-serviceaccount
      containers:
      - name: ray-head
        image: ray-training-laptop:latest
        imagePullPolicy: IfNotPresent
        command:
        - /bin/bash
        - -c
        - |
          echo "Starting Ray head node with GPU support..."
          nvidia-smi || echo "WARNING: nvidia-smi failed"
          python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')" || true
          
          ray start --head \
            --port=6379 \
            --ray-client-server-port=10001 \
            --dashboard-host=0.0.0.0 \
            --dashboard-port=8265 \
            --include-dashboard=true \
            --num-cpus=8 \
            --num-gpus=1 \
            --object-store-memory=2000000000 \
            --disable-usage-stats \
            --block
        ports:
        - containerPort: 6379
          name: redis
        - containerPort: 8265
          name: dashboard
        - containerPort: 10001
          name: client
        - containerPort: 8080
          name: metrics
        envFrom:
        - configMapRef:
            name: ray-config
        env:
        - name: RAY_MEMORY_MONITOR_REFRESH_MS
          value: "0"
        - name: NVIDIA_VISIBLE_DEVICES
          value: "all"
        - name: NVIDIA_DRIVER_CAPABILITIES
          value: "compute,utility"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: PYTORCH_CUDA_ALLOC_CONF
          value: "max_split_size_mb:512"
        - name: OMP_NUM_THREADS
          value: "4"
        resources:
          requests:
            cpu: "4"
            memory: "8Gi"
            nvidia.com/gpu: "1"
          limits:
            cpu: "8"
            memory: "16Gi"
            nvidia.com/gpu: "1"
        volumeMounts:
        - name: shared-data
          mountPath: /mnt/data
        - name: checkpoints
          mountPath: /mnt/checkpoints
        - name: dshm
          mountPath: /dev/shm
      volumes:
      - name: shared-data
        hostPath:
          path: /tmp/ray-data
          type: DirectoryOrCreate
      - name: checkpoints
        hostPath:
          path: /tmp/ray-checkpoints
          type: DirectoryOrCreate
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 4Gi
