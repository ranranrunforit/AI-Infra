---
# Kubernetes Job for running a training session
# Connects to the Ray head and submits training work
#
# Usage:
#   kubectl apply -f kubernetes/training-job.yaml
#   kubectl logs -f jobs/training-job -n ray-cluster

apiVersion: batch/v1
kind: Job
metadata:
  name: training-job
  namespace: ray-cluster
spec:
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: training
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
    spec:
      serviceAccountName: ray-serviceaccount
      restartPolicy: OnFailure
      containers:
      - name: trainer
        image: ray-training-laptop:latest
        imagePullPolicy: IfNotPresent
        command:
        - /bin/bash
        - -c
        - |
          echo "=== Starting Training Job ==="
          echo "Checking GPU availability..."
          python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, GPUs: {torch.cuda.device_count()}')"
          
          echo "Connecting to Ray cluster..."
          # Backend defaults to 'nccl' (GPU), use --backend gloo for CPU fallback or Windows issues
          python -m src.training.distributed_trainer \
            --model resnet18 \
            --dataset cifar10 \
            --epochs 10 \
            --batch-size 128 \
            --learning-rate 0.01 \
            --mixed-precision fp16 \
            --data-path /mnt/data \
            --checkpoint-dir /mnt/checkpoints \
            --ray-address ray://ray-head:10001 \
            --prometheus-port 8080 \
            --backend nccl
          
          echo "=== Training Complete ==="
        ports:
        - containerPort: 8080
          name: metrics
        env:
        - name: RAY_ADDRESS
          value: "ray://ray-head:10001"
        - name: PYTHONPATH
          value: "/app/src:/app"
        - name: PYTHONUNBUFFERED
          value: "1"
        - name: NVIDIA_VISIBLE_DEVICES
          value: "all"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        resources:
          requests:
            cpu: "2"
            memory: "4Gi"
            nvidia.com/gpu: "1"
          limits:
            cpu: "4"
            memory: "8Gi"
            nvidia.com/gpu: "1"
        volumeMounts:
        - name: shared-data
          mountPath: /mnt/data
        - name: checkpoints
          mountPath: /mnt/checkpoints
      volumes:
      - name: shared-data
        hostPath:
          path: /tmp/ray-data
          type: DirectoryOrCreate
      - name: checkpoints
        hostPath:
          path: /tmp/ray-checkpoints
          type: DirectoryOrCreate