"""
End-to-End Integration Tests for Production ML System
======================================================

These tests verify the complete workflow from API request to response,
including all integrated components (API, model serving, monitoring).

Run these tests in CI/CD after deployment to staging/production.

Author: AI Infrastructure Curriculum
"""

import os
import pytest
import requests
import time
from typing import Dict, Any
from pathlib import Path

# TODO: Import additional libraries
from PIL import Image
import io
import json

# ============================================================================
# CONFIGURATION
# ============================================================================

# TODO: Load configuration from environment variables
API_URL = os.getenv('API_URL', 'http://localhost:5000')
API_KEY = os.getenv('API_KEY', 'test-api-key')
TIMEOUT = int(os.getenv('REQUEST_TIMEOUT', '30'))

# TODO: Define test data paths
TEST_DATA_DIR = Path(__file__).parent.parent / 'data'


# ============================================================================
# FIXTURES
# ============================================================================

@pytest.fixture(scope='session')
def api_client():
    """
    Create a configured API client for tests.

    Returns:
        dict: Configuration for API requests
    """
    # TODO: Implement API client configuration
    return {
        'base_url': API_URL,
        'headers': {
            'X-API-Key': API_KEY
        },
        'timeout': TIMEOUT
    }


@pytest.fixture(scope='session')
def test_image():
    """
    Load a test image for prediction requests.

    Returns:
        bytes: Test image data
    """
    # TODO: Load test image
    image_path = TEST_DATA_DIR / 'test_image.jpg'
    with open(image_path, 'rb') as f:
        return f.read()
    


# ============================================================================
# HEALTH CHECK TESTS
# ============================================================================

class TestHealthChecks:
    """Test health check endpoints"""

    def test_health_endpoint_accessible(self, api_client):
        """
        Test that the /health endpoint is accessible.

        Expected: 200 OK response
        """
        # TODO: Implement health check test
        response = requests.get(
            f"{api_client['base_url']}/health",
            timeout=api_client['timeout']
        )
        assert response.status_code == 200
        assert response.json()['status'] == 'healthy'
        

    def test_health_response_time(self, api_client):
        """
        Test that health check responds quickly.

        Expected: Response time < 100ms
        """
        # TODO: Implement response time test
        start = time.time()
        response = requests.get(
            f"{api_client['base_url']}/health",
            timeout=api_client['timeout']
        )
        duration = time.time() - start
        
        assert response.status_code == 200
        assert duration < 0.1, f"Health check took {duration}s, expected <0.1s"
        

    def test_health_returns_model_info(self, api_client):
        """
        Test that health check includes model information.

        Expected: Model name and version in response
        """
        # TODO: Implement model info test
        response = requests.get(
            f"{api_client['base_url']}/health",
            timeout=api_client['timeout']
        )
        
        assert response.status_code == 200
        data = response.json()
        assert 'model' in data
        assert 'name' in data['model']
        assert 'version' in data['model']
        


# ============================================================================
# AUTHENTICATION TESTS
# ============================================================================

class TestAuthentication:
    """Test API authentication and authorization"""

    def test_missing_api_key_rejected(self, api_client):
        """
        Test that requests without API key are rejected.

        Expected: 401 Unauthorized
        """
        # TODO: Implement authentication test
        response = requests.post(
            f"{api_client['base_url']}/predict",
            timeout=api_client['timeout']
        )
        assert response.status_code == 401
        

    def test_invalid_api_key_rejected(self, api_client):
        """
        Test that requests with invalid API key are rejected.

        Expected: 403 Forbidden
        """
        # TODO: Implement invalid key test
        response = requests.post(
            f"{api_client['base_url']}/predict",
            headers={'X-API-Key': 'invalid-key'},
            timeout=api_client['timeout']
        )
        assert response.status_code == 403
        

    def test_valid_api_key_accepted(self, api_client, test_image):
        """
        Test that requests with valid API key are accepted.

        Expected: Request processed (may fail for other reasons, but not auth)
        """
        # TODO: Implement valid key test
        files = {'file': ('test.jpg', test_image, 'image/jpeg')}
        response = requests.post(
            f"{api_client['base_url']}/predict",
            headers=api_client['headers'],
            files=files,
            timeout=api_client['timeout']
        )
        assert response.status_code != 401
        assert response.status_code != 403
        


# ============================================================================
# PREDICTION ENDPOINT TESTS
# ============================================================================

class TestPredictionEndpoint:
    """Test the main prediction functionality"""

    def test_predict_with_valid_image(self, api_client, test_image):
        """
        Test prediction with a valid image.

        Expected: 200 OK with predictions
        """
        # TODO: Implement prediction test
        files = {'file': ('test.jpg', test_image, 'image/jpeg')}
        response = requests.post(
            f"{api_client['base_url']}/predict",
            headers=api_client['headers'],
            files=files,
            timeout=api_client['timeout']
        )
        
        assert response.status_code == 200
        data = response.json()
        assert 'predictions' in data
        assert len(data['predictions']) > 0
        assert 'model_version' in data
        

    def test_predict_response_format(self, api_client, test_image):
        """
        Test that prediction response has expected format.

        Expected: Predictions with class labels and confidence scores
        """
        # TODO: Implement response format test
        files = {'file': ('test.jpg', test_image, 'image/jpeg')}
        response = requests.post(
            f"{api_client['base_url']}/predict",
            headers=api_client['headers'],
            files=files,
            timeout=api_client['timeout']
        )
        
        assert response.status_code == 200
        data = response.json()
        
        # Verify prediction structure
        prediction = data['predictions'][0]
        assert 'class' in prediction
        assert 'confidence' in prediction
        assert 0 <= prediction['confidence'] <= 1
        

    def test_predict_latency_slo(self, api_client, test_image):
        """
        Test that prediction latency meets SLO.

        Expected: P95 latency < 500ms
        """
        # TODO: Implement latency test
        Run multiple predictions and check P95 latency
        latencies = []
        for _ in range(20):
            files = {'file': ('test.jpg', test_image, 'image/jpeg')}
            start = time.time()
            response = requests.post(
                f"{api_client['base_url']}/predict",
                headers=api_client['headers'],
                files=files,
                timeout=api_client['timeout']
            )
            latency = time.time() - start
            latencies.append(latency)
            assert response.status_code == 200
        
        # Calculate P95
        latencies.sort()
        p95 = latencies[int(len(latencies) * 0.95)]
        assert p95 < 0.5, f"P95 latency {p95}s exceeds 500ms SLO"
        

    def test_predict_without_file(self, api_client):
        """
        Test prediction without uploading a file.

        Expected: 400 Bad Request
        """
        # TODO: Implement no-file test
        response = requests.post(
            f"{api_client['base_url']}/predict",
            headers=api_client['headers'],
            timeout=api_client['timeout']
        )
        assert response.status_code == 400
        

    def test_predict_with_invalid_file_type(self, api_client):
        """
        Test prediction with invalid file type (not an image).

        Expected: 400 Bad Request
        """
        # TODO: Implement invalid file type test
        files = {'file': ('test.txt', b'not an image', 'text/plain')}
        response = requests.post(
            f"{api_client['base_url']}/predict",
            headers=api_client['headers'],
            files=files,
            timeout=api_client['timeout']
        )
        assert response.status_code == 400
        

    def test_predict_with_large_file(self, api_client):
        """
        Test prediction with file exceeding size limit.

        Expected: 400 Bad Request (file too large)
        """
        # TODO: Implement large file test
        # Create 15MB file (assuming 10MB limit)
        large_file = b'0' * (15 * 1024 * 1024)
        files = {'file': ('large.jpg', large_file, 'image/jpeg')}
        response = requests.post(
            f"{api_client['base_url']}/predict",
            headers=api_client['headers'],
            files=files,
            timeout=api_client['timeout']
        )
        assert response.status_code == 400
        


# ============================================================================
# INFO ENDPOINT TESTS
# ============================================================================

class TestInfoEndpoint:
    """Test the /info endpoint"""

    def test_info_endpoint_accessible(self, api_client):
        """
        Test that /info endpoint is accessible with authentication.

        Expected: 200 OK with service and model information
        """
        # TODO: Implement info endpoint test
        response = requests.get(
            f"{api_client['base_url']}/info",
            headers=api_client['headers'],
            timeout=api_client['timeout']
        )
        
        assert response.status_code == 200
        data = response.json()
        assert 'service' in data
        assert 'model' in data
        

    def test_info_includes_model_version(self, api_client):
        """
        Test that /info includes current model version.

        Expected: Model version information present
        """
        # TODO: Implement model version test
        response = requests.get(
            f"{api_client['base_url']}/info",
            headers=api_client['headers'],
            timeout=api_client['timeout']
        )
        
        assert response.status_code == 200
        data = response.json()
        assert 'model' in data
        assert 'version' in data['model']
        assert data['model']['version'] is not None
        


# ============================================================================
# METRICS ENDPOINT TESTS
# ============================================================================

class TestMetricsEndpoint:
    """Test Prometheus metrics endpoint"""

    def test_metrics_endpoint_accessible(self, api_client):
        """
        Test that /metrics endpoint is accessible.

        Expected: 200 OK with Prometheus-formatted metrics
        """
        # TODO: Implement metrics endpoint test
        response = requests.get(
            f"{api_client['base_url']}/metrics",
            timeout=api_client['timeout']
        )
        
        assert response.status_code == 200
        assert 'text/plain' in response.headers.get('Content-Type', '')
        

    def test_metrics_include_custom_metrics(self, api_client):
        """
        Test that custom metrics are exposed.

        Expected: ML-specific metrics present
        """
        # TODO: Implement custom metrics test
        response = requests.get(
            f"{api_client['base_url']}/metrics",
            timeout=api_client['timeout']
        )
        
        assert response.status_code == 200
        metrics_text = response.text
        
        # Check for custom metrics
        assert 'http_requests_total' in metrics_text
        assert 'http_request_duration_seconds' in metrics_text
        assert 'model_predictions_total' in metrics_text
        


# ============================================================================
# LOAD TESTING
# ============================================================================

class TestLoadHandling:
    """Test system behavior under load"""

    @pytest.mark.slow
    def test_concurrent_requests(self, api_client, test_image):
        """
        Test handling of concurrent requests.

        Expected: All requests succeed (or degrade gracefully)
        """
        # TODO: Implement concurrent requests test
        import concurrent.futures
        
        def send_request():
            files = {'file': ('test.jpg', test_image, 'image/jpeg')}
            return requests.post(
                f"{api_client['base_url']}/predict",
                headers=api_client['headers'],
                files=files,
                timeout=api_client['timeout']
            )
        
        # Send 50 concurrent requests
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(send_request) for _ in range(50)]
            results = [f.result() for f in concurrent.futures.as_completed(futures)]
        
        # Check success rate
        success_count = sum(1 for r in results if r.status_code == 200)
        success_rate = success_count / len(results)
        assert success_rate >= 0.95, f"Success rate {success_rate} below 95%"
        

    @pytest.mark.slow
    def test_sustained_load(self, api_client, test_image):
        """
        Test handling of sustained load over time.

        Expected: System remains stable, no degradation
        """
        # TODO: Implement sustained load test
        Run requests for 2 minutes, check no degradation
        duration = 120  # seconds
        start_time = time.time()
        latencies = []
        
        while time.time() - start_time < duration:
            files = {'file': ('test.jpg', test_image, 'image/jpeg')}
            req_start = time.time()
            response = requests.post(
                f"{api_client['base_url']}/predict",
                headers=api_client['headers'],
                files=files,
                timeout=api_client['timeout']
            )
            latency = time.time() - req_start
            latencies.append(latency)
        
            assert response.status_code == 200
            time.sleep(0.1)  # 10 req/sec
        
        # Check no significant degradation
        early_p95 = sorted(latencies[:100])[95]
        late_p95 = sorted(latencies[-100:])[95]
        assert late_p95 < early_p95 * 1.5, "Latency degraded over time"
        


# ============================================================================
# INTEGRATION WITH MONITORING
# ============================================================================

class TestMonitoringIntegration:
    """Test integration with monitoring systems"""

    def test_requests_counted_in_prometheus(self, api_client, test_image):
        """
        Test that requests are counted in Prometheus metrics.

        Expected: Metrics increase after making requests
        """
        # TODO: Implement Prometheus integration test
        # Get initial metric value
        response = requests.get(f"{api_client['base_url']}/metrics")
        initial_metrics = response.text
        
        # Make a prediction request
        files = {'file': ('test.jpg', test_image, 'image/jpeg')}
        requests.post(
            f"{api_client['base_url']}/predict",
            headers=api_client['headers'],
            files=files
        )
        
        # Get updated metric value
        response = requests.get(f"{api_client['base_url']}/metrics")
        updated_metrics = response.text
        
        # Verify metrics changed
        assert updated_metrics != initial_metrics
        


# ============================================================================
# ERROR HANDLING TESTS
# ============================================================================

class TestErrorHandling:
    """Test error handling and graceful degradation"""

    def test_handles_model_unavailable(self, api_client):
        """
        Test system behavior when model is unavailable.

        Expected: Graceful error message, not 500 crash
        """
        # TODO: Implement error handling test
        # This would require a way to simulate model unavailability
        pass

    def test_error_responses_dont_leak_info(self, api_client):
        """
        Test that error messages don't leak sensitive information.

        Expected: Generic error messages, no stack traces
        """
        # TODO: Implement security test
        # Make invalid request
        response = requests.post(
            f"{api_client['base_url']}/predict",
            headers={'X-API-Key': 'invalid'},
            timeout=api_client['timeout']
        )
        
        error_message = response.json().get('error', '')
        # Should not contain stack traces, file paths, etc.
        assert 'Traceback' not in error_message
        assert '/home/' not in error_message
        assert 'Exception' not in error_message
        


# ==============================================================================
# TEST EXECUTION NOTES
# ==============================================================================

"""
TODO: Run these tests

Local testing:
    export API_URL=http://localhost:5000
    export API_KEY=test-key
    pytest tests/integration/test_e2e.py -v

CI/CD testing (staging):
    export API_URL=https://staging.example.com
    export API_KEY=$STAGING_API_KEY
    pytest tests/integration/test_e2e.py -v

CI/CD testing (production):
    export API_URL=https://api.example.com
    export API_KEY=$PRODUCTION_API_KEY
    pytest tests/integration/test_e2e.py -v --skip-slow

Run only fast tests:
    pytest tests/integration/test_e2e.py -v -m "not slow"

Generate HTML report:
    pytest tests/integration/test_e2e.py --html=report.html

Expected test duration:
- Fast tests: ~30 seconds
- All tests including load tests: ~5 minutes
"""
