FROM apache/airflow:2.9.0-python3.11

# Project 07: End-to-End MLOps Pipeline
# Airflow Docker image with custom dependencies

USER root

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    git \
    curl \
    vim \
    netcat-traditional \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

USER airflow

# Copy requirements file first (for better layer caching)
COPY --chown=airflow:airflow requirements.txt /opt/airflow/requirements.txt

# Install Python dependencies
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r /opt/airflow/requirements.txt

# Install additional Airflow providers
RUN pip install --no-cache-dir \
    apache-airflow-providers-docker \
    apache-airflow-providers-postgres \
    apache-airflow-providers-http

# Set working directory
WORKDIR /opt/airflow

# Create necessary directories
RUN mkdir -p /opt/airflow/dags \
    /opt/airflow/logs \
    /opt/airflow/plugins \
    /opt/airflow/data/raw \
    /opt/airflow/data/processed \
    /opt/airflow/models

# Copy source code and DAGs
COPY --chown=airflow:airflow src/ /opt/airflow/src/
COPY --chown=airflow:airflow dags/ /opt/airflow/dags/

# Set PYTHONPATH to include src directory
ENV PYTHONPATH="${PYTHONPATH}:/opt/airflow:/opt/airflow/src"

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD airflow jobs check --job-type SchedulerJob --hostname "${HOSTNAME}" || exit 1
